{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# %pip install scrapegraphai\n",
    "%pip install scrapegraphai==1.20.0b1\n",
    "%apt install chromium-chromedriver\n",
    "%pip install nest_asyncio\n",
    "%pip install playwright\n",
    "%playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rateria/anaconda3/envs/cs-5787/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# sanity\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(\"categorized_content_links.csv\")\n",
    "\n",
    "column_name = \"url\"  # replace with the actual column name\n",
    "urls = df[column_name].dropna().unique().tolist()\n",
    "\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model schema\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubHealthSchema(BaseModel):\n",
    "    supporting_claim: str\n",
    "    contradictory_claim: str\n",
    "    ambiguous_claim: str # 0: +ve, 1: -ve, 2: ambiguous \n",
    "    \n",
    "    def to_json(self):\n",
    "        return {\"supporting_claim\": self.supporting_claim, \"contradictory_claim\": self.contradictory_claim, \"ambiguous_claim\": self.ambiguous_claim}\n",
    "    \n",
    "class Schema(BaseModel):\n",
    "    claims: List[PubHealthSchema]\n",
    "    \n",
    "    def to_json(self):\n",
    "        return [data.to_json() for data in self.claims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Use the data from this url to generate a group of claims and evidences according to the description below:\n",
    "\n",
    "Claim: A claim is an assertion or statement that proposes an idea, fact, or opinion. It is often the subject of verification, as it can contain elements that may be true, false, exaggerated, or misleading. Claims may require evidence or further investigation to determine their accuracy.\n",
    "\n",
    "Evidence: The evidence provides context, evidence, and clarification regarding the claim. It may include a breakdown of factual elements, highlight any inaccuracies, and explain nuances that help the reader understand the validity or implications of the claim. Explanations aim to inform by distinguishing between what is supported by evidence and what may be incorrect or misleading.\n",
    "\n",
    "Label: The label is a categorization of the claim after it has been evaluated. It often indicates the nature or degree of accuracy of the claim, such as whether it is true, false, partially true, misleading, or speculative. Labels help in classifying claims for easier identification and understanding of their reliability.\n",
    " \n",
    "Ensure that each claim is only one sentence long. Ensure that each evidence is NOT more than 4 sentences long. The values of the labels can be 0 or 1. 0 is when the evidence SUPPORTS the claim and 1 is when the evidence disproves the claim. Only generate claims which have the label of 1 for now.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Schema\ndata\n  Field required [type=missing, input_value={'src': 'https://sniv3r2....ions-to-seminal-plasma'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m graph_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: open_ai_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheadless\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m smart_scraper_graph \u001b[38;5;241m=\u001b[39m SmartScraperGraph(\n\u001b[1;32m     13\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     source\u001b[38;5;241m=\u001b[39murls[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     15\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m     16\u001b[0m     config\u001b[38;5;241m=\u001b[39mgraph_config\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m result \u001b[38;5;241m=\u001b[39m smart_scraper_graph\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Code/cs-5787-final-project/.conda/lib/python3.9/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Schema\ndata\n  Field required [type=missing, input_value={'src': 'https://sniv3r2....ions-to-seminal-plasma'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing"
     ]
    }
   ],
   "source": [
    "graph_config = {\n",
    "    \"llm\": {\n",
    "        \"api_key\": open_ai_key,\n",
    "        \"model\": \"openai/gpt-4o-mini\",\n",
    "    },\n",
    "    \"verbose\": True,\n",
    "    \"headless\": False,\n",
    "}\n",
    "\n",
    "schema = Schema(src=urls[0])\n",
    "\n",
    "smart_scraper_graph = SmartScraperGraph(\n",
    "    prompt=\"prompt\",\n",
    "    source=urls[0],\n",
    "    schema=schema,\n",
    "    config=graph_config\n",
    ")\n",
    "\n",
    "result = smart_scraper_graph.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_schema_to_csv(schema, file_name: str):    \n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(schema['data'])\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_schema_to_csv(result, \"test-1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(num_samples):\n",
    "    prompt = f\"\"\"You are a medical expert. You will be given a medical document, generate three types of claims:\\n\\n1) A supporting claim that paraphrases a key assertion.\\n2) A contradictory claim that directly contradicts a key evidence provided in the summary.\\n3) An ambiguous claim that either partially supports or contradicts, or presents elements that are neither clearly supported nor contradicted.\\n\\nEach claim should be one or two sentences long. Ideally, the claims should be generated from different key assertions or sections of the summary.\\n\\nReturn ONLY the claims in this exact JSON format below. DO NOT include any extra text or explanations. DO NOT add ```json``` formatting. Just output the exact JSON as a string.\\n[{{\\n  \\\"supporting_claim\\\": '...',\\n  \\\"contradictory_claim\\\": '...',\\n  \\\"ambiguous_claim\\\": '...'\\n}}].\\n\\n Output {num_samples} triplets of supporting, contradictory, and ambiguous claims from the provided summary. Give all {num_samples} triplets in the same JSON array.\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=open_ai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"./uptodate/1.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": get_prompt(20)},\n",
    "        {\"role\": \"user\", \"content\": f\"Document:\\n\\n{text}\"}\n",
    "    ],\n",
    "    response_format=Schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'supporting_claim': 'UpToDate content is not meant to replace professional medical advice and should only be used as a supplementary source of information.',\n",
       "  'contradictory_claim': 'UpToDate is intended to provide definitive medical advice that can be solely relied upon for treatment decisions.',\n",
       "  'ambiguous_claim': 'While UpToDate offers extensive information on birth control methods, the effectiveness of these methods can vary significantly depending on individual circumstances.'},\n",
       " {'supporting_claim': 'IUDs and implants are among the most effective birth control methods due to their low risk of failure.',\n",
       "  'contradictory_claim': 'Contraceptives such as the condom and diaphragm are just as effective as long-term methods like IUDs.',\n",
       "  'ambiguous_claim': 'The effectiveness of birth control methods can greatly depend on how consistently and correctly they are used, leaving some uncertainty regarding their overall reliability.'},\n",
       " {'supporting_claim': 'Emergency contraception can be used up to five days after unprotected intercourse to decrease the chance of pregnancy.',\n",
       "  'contradictory_claim': 'Emergency contraception is only effective if used within 12 hours after unprotected sex.',\n",
       "  'ambiguous_claim': 'The availability of emergency contraception varies greatly depending on local regulations and individual pharmacy practices.'},\n",
       " {'supporting_claim': 'Consistent use of contraceptive methods is essential for maximizing their effectiveness in preventing pregnancy.',\n",
       "  'contradictory_claim': 'Utilizing any birth control method guarantees 100% protection against pregnancy if employed occasionally.',\n",
       "  'ambiguous_claim': 'While some contraceptive methods are reversible, the degree of effectiveness may change based on personal health conditions.'},\n",
       " {'supporting_claim': 'Birth control choices depend on personal and partner acceptability as well as potential side effects.',\n",
       "  'contradictory_claim': 'The choice of birth control method is entirely independent of side effects and personal preferences.',\n",
       "  'ambiguous_claim': 'Some women report more severe side effects with hormonal methods when compared to non-hormonal options, but experiences can differ widely.'},\n",
       " {'supporting_claim': 'Factors like cost, convenience, and side effects should all be considered when choosing a birth control method.',\n",
       "  'contradictory_claim': 'The cost of birth control methods has no impact on their effectiveness or accessibility.',\n",
       "  'ambiguous_claim': 'While methods like condoms and pills are widely available, how accessible they are can depend heavily on individual circumstances.'},\n",
       " {'supporting_claim': 'The birth control patch is as effective as traditional oral contraceptives but might be preferred for its ease of use.',\n",
       "  'contradictory_claim': 'Birth control patches are less effective than birth control pills, regardless of consistent usage.',\n",
       "  'ambiguous_claim': 'Although the birth control patch is recommended for its convenience, some women might have concerns about potential skin reactions.'},\n",
       " {'supporting_claim': 'Tubal ligation is a common procedure that provides permanent contraception for women.',\n",
       "  'contradictory_claim': 'Sterilization methods do not provide definitive long-term contraception and can be easily reversed.',\n",
       "  'ambiguous_claim': 'Sterilization is permanent, yet some women may experience regrets later on, leading to discussions about alternatives.'},\n",
       " {'supporting_claim': 'IUDs can be removed at any time, offering flexibility for women regarding their contraceptive choices.',\n",
       "  'contradictory_claim': 'Once an IUD is inserted, it cannot be removed until it naturally dislodges after many years.',\n",
       "  'ambiguous_claim': 'While IUDs are highly effective, user satisfaction may depend on managing potential discomfort during insertion.'},\n",
       " {'supporting_claim': 'Emergency contraceptive pills are less effective than the IUD when it comes to preventing pregnancy after unprotected intercourse.',\n",
       "  'contradictory_claim': 'Emergency contraceptive pills work as effectively as IUDs with no significant difference in outcomes.',\n",
       "  'ambiguous_claim': 'Despite being widely accessible, some women feel uncomfortable discussing or obtaining emergency contraception.'},\n",
       " {'supporting_claim': 'Barrier methods, like condoms, can also help reduce the transmission of sexually transmitted diseases.',\n",
       "  'contradictory_claim': 'Barrier methods do not offer any protection against STIs and should not be relied upon for that purpose.',\n",
       "  'ambiguous_claim': 'The effectiveness of barrier methods might be influenced by user technique and adherence to instructions.'},\n",
       " {'supporting_claim': 'Contrary to other forms of contraception, the birth control implant can be effective within 24 hours of insertion.',\n",
       "  'contradictory_claim': 'The birth control implant offers no protection against pregnancy until it has been in place for at least one month.',\n",
       "  'ambiguous_claim': 'While the birth control implant is known for its effectiveness, the experience of side effects can vary significantly among users.'},\n",
       " {'supporting_claim': 'Hormonal birth control methods can lead to a reduction in menstrual bleeding for many women.',\n",
       "  'contradictory_claim': 'Hormonal contraception always increases menstrual bleeding in every user without exception.',\n",
       "  'ambiguous_claim': 'Some women may prefer hormonal methods for the menstrual benefits, but may also experience unexpected side effects, leading to mixed reviews.'},\n",
       " {'supporting_claim': 'Consulting with a healthcare provider is essential for a thorough understanding of birth control options.',\n",
       "  'contradictory_claim': 'Reliable information on birth control methods can be exclusively found online without the need for professional guidance.',\n",
       "  'ambiguous_claim': 'The accessibility of professional medical advice can vary widely, influencing the decision-making process on birth control methods.'},\n",
       " {'supporting_claim': 'Studies indicate that many women stop having periods entirely after using certain IUDs for a year.',\n",
       "  'contradictory_claim': 'Using IUDs consistently guarantees that all women will continue to have regular menstrual cycles.',\n",
       "  'ambiguous_claim': 'While many report stopping menstruation with an IUD, individual hormonal responses can lead to differing experiences.'},\n",
       " {'supporting_claim': 'Women using medroxyprogesterone acetate (DMPA) may experience irregular bleeding as a common side effect during the first months.',\n",
       "  'contradictory_claim': 'DMPA users do not report any side effects, including irregular bleeding or spotting.',\n",
       "  'ambiguous_claim': 'Although DMPA has proven effectiveness, the inconsistency of menstrual cycles can be a significant concern for some women.'},\n",
       " {'supporting_claim': 'The content on UpToDate assists individuals in making informed choices regarding their birth control options.',\n",
       "  'contradictory_claim': 'Consulting UpToDate is sufficient to make definitive decisions on contraceptive methods without additional resources.',\n",
       "  'ambiguous_claim': 'While UpToDate provides a wealth of information on birth control, the interpretation of this information may vary widely among different users.'},\n",
       " {'supporting_claim': 'Natural family planning methods require careful monitoring of physiological changes to accurately identify fertile days.',\n",
       "  'contradictory_claim': 'Natural family planning methods can be employed without any monitoring or knowledge of menstrual cycles.',\n",
       "  'ambiguous_claim': 'The reliance on natural family planning could make it less effective for those with irregular cycles, raising questions about its reliability.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.parsed.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from functools import partial\n",
    "from multiprocessing import Lock, Manager, Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.agents import AgentExecutor, Tool, initialize_agent\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.agent_toolkits import JsonToolkit, create_json_agent\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_community.tools.json.tool import JsonSpec\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sn/h8k80m7j0wx704gsf17nmqmm0000gn/T/ipykernel_95238/4272845173.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  oaiembeds = OpenAIEmbeddings(openai_api_key=\"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\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "oaiembeds = OpenAIEmbeddings(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunks(filepath):\n",
    "    with open(filepath) as f:\n",
    "        text = f.read()\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    # print (f\"{len(single_sentences_list)} senteneces were found\")\n",
    "    sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
    "    sentences = combine_sentences(sentences)\n",
    "    embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    distances, sentences = calculate_cosine_distances(sentences)\n",
    "    breakpoint_percentile_threshold = 90\n",
    "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "\n",
    "    # Then we'll see how many distances are actually above this one\n",
    "    num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "    # plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f\"{num_distances_above_theshold + 1} Chunks\");\n",
    "\n",
    "    # Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "    # Initialize the start index\n",
    "    start_index = 0\n",
    "\n",
    "    # Create a list to hold the grouped sentences\n",
    "    chunks = []\n",
    "\n",
    "    # Iterate through the breakpoints to slice the sentences\n",
    "    for index in indices_above_thresh:\n",
    "        # The end index is the current breakpoint\n",
    "        end_index = index\n",
    "\n",
    "        # Slice the sentence_dicts from the current start index to the end index\n",
    "        group = sentences[start_index:end_index + 1]\n",
    "        combined_text = ' '.join([d['sentence'] for d in group])\n",
    "        chunks.append(combined_text)\n",
    "        \n",
    "        # Update the start index for the next group\n",
    "        start_index = index + 1\n",
    "\n",
    "    # The last group, if any sentences remain\n",
    "    if start_index < len(sentences):\n",
    "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "        chunks.append(combined_text)\n",
    "        \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = generate_chunks(\"./UTD2txt/10.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_files(output_file, log_file):\n",
    "    \"\"\"Initialize output and log files with headers if they don't exist.\"\"\"\n",
    "    if not os.path.exists(output_file):\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['Topic', 'Evidence', 'Supports', 'Contradicts', 'Ambiguous'])\n",
    "            writer.writeheader()\n",
    "    \n",
    "    if not os.path.exists(log_file):\n",
    "        with open(log_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['Processed_Topic'])\n",
    "            writer.writeheader()\n",
    "\n",
    "# %%\n",
    "# File paths\n",
    "log_file = os.path.join('.', 'csv', 'uptodate_process_log.csv')\n",
    "output_file = os.path.join('.', 'csv', 'claim_triplets_uptodate.csv')\n",
    "input_file = os.path.join('.', 'csv', 'categorized_content_links_unique.csv')\n",
    "\n",
    "# OpenAI API key\n",
    "# openai_api_key = \"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\"\n",
    "# openai_api_key = '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'\n",
    "openai_api_key = \"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\"\n",
    "# API key for OpenAI (Harshini)\n",
    "# openai_api_key = \"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\"\n",
    "# openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize files\n",
    "init_files(output_file, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_row(file_path, row_dict, file_lock):\n",
    "    \"\"\"Write a single row to the CSV file in a thread-safe manner.\"\"\"\n",
    "    with file_lock:\n",
    "        file_exists = os.path.exists(file_path)\n",
    "        mode = 'a' if file_exists else 'w'\n",
    "        \n",
    "        with open(file_path, mode, newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(row_dict.keys()))\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_claim_triplet(summary):\n",
    "    \"\"\"Generate claim triplets from a summary.\"\"\"\n",
    "    try:\n",
    "        prompt = \"\"\"\n",
    "        Using the following detailed summary, generate three types of claims:\\n\\n1) A supporting claim that paraphrases a key assertion.\\n2) A contradictory claim that directly contradicts a key evidence provided in the summary.\\n3) An ambiguous claim that either partially supports or contradicts, or presents elements that are neither clearly supported nor contradicted.\\n\\nEach claim should be one or two sentences long. Ideally, the claims should be generated from different key assertions or sections of the summary.\\n\\nReturn ONLY the claims in this exact JSON format below. DO NOT include any extra text or explanations. DO NOT add ```json``` formatting. Just output the exact JSON as a string.\\n[{{\\n  \\\"supporting_claim\\\": '...',\\n  \\\"contradictory_claim\\\": '...',\\n  \\\"ambiguous_claim\\\": '...'\\n}}].\\n\\n Output {num_samples} triplets of supporting, contradictory, and ambiguous claims from the provided summary. Give all {num_samples} triplets in the same JSON array.\\n\\n\\n\\n\n",
    "\n",
    "        Summary:\n",
    "        {summary}\n",
    "        \"\"\"\n",
    "        # response = agent.run(prompt)\n",
    "        llm = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"summary\", \"additional_details\"],\n",
    "            template=prompt\n",
    "        )\n",
    "        # structured = llm.with_structured_output(Claims)\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        response = llm_chain.run({\"summary\": summary, \"num_samples\": 4})\n",
    "        \n",
    "        print(\"JSON Response\", response)\n",
    "        \n",
    "        response = json.loads(response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating claims for summary: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Response [{\n",
      "  \"supporting_claim\": \"The timing of antiretroviral therapy (ART) is crucial to reduce the incidence of immune reconstitution inflammatory syndrome (IRIS) among patients with tuberculosis (TB).\",\n",
      "  \"contradictory_claim\": \"IRIS occurs in fewer than 8 percent of patients with TB, contradicting the reported prevalence of 8 to 43 percent.\",\n",
      "  \"ambiguous_claim\": \"While management of TB-IRIS is similar to that of IRIS from other conditions, the specific details of treatment differences remain unclear.\"\n",
      "}, {\n",
      "  \"supporting_claim\": \"Patients with an initial CD4 count below 100/microL face a higher risk of developing IRIS.\",\n",
      "  \"contradictory_claim\": \"Patients with a significant reduction in viral load and an increase in CD4 count do not experience an increased risk of IRIS, which contradicts the evidence presented.\",\n",
      "  \"ambiguous_claim\": \"The clinical manifestations of IRIS can vary widely, leading to some uncertainty about whether all symptoms are directly linked to TB.\"\n",
      "}, {\n",
      "  \"supporting_claim\": \"Management strategies for TB-IRIS do not significantly differ from those for IRIS associated with other medical conditions.\",\n",
      "  \"contradictory_claim\": \"The assertion that IRIS is usually self-limited contradicts the notion that it can sometimes necessitate changes to treatment regimens.\",\n",
      "  \"ambiguous_claim\": \"The use of nonsteroidal anti-inflammatory drugs may provide relief for IRIS symptoms, but their effectiveness compared to corticosteroids remains debated.\"\n",
      "}, {\n",
      "  \"supporting_claim\": \"Clinical symptoms of IRIS can include persistent pyrexia and worsening pulmonary infiltrates in HIV-infected patients with TB.\",\n",
      "  \"contradictory_claim\": \"The summary suggests that IRIS does not typically cause any symptoms, which contradicts the detailed manifestations listed.\",\n",
      "  \"ambiguous_claim\": \"Although IRIS is often self-limited, the extent to which this applies to all patients with TB remains uncertain.\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "out = generate_claim_triplet(chunks[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The timing of antiretroviral therapy (ART) is crucial to reduce the incidence of immune reconstitution inflammatory syndrome (IRIS) among patients with tuberculosis (TB).'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].get(\"supporting_claim\", \"na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Manager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mManager\u001b[49m()\n\u001b[1;32m      4\u001b[0m file_lock \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m      6\u001b[0m log_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(log_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Manager' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "manager = Manager()\n",
    "file_lock = manager.Lock()\n",
    "\n",
    "log_df = pd.read_csv(log_file)\n",
    "try:\n",
    "    processed_files = set(log_df[\"processed_file\"])\n",
    "except:\n",
    "    processed_files = []\n",
    "\n",
    "folder_path = './UTD2txt'\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if file_path in processed_files:\n",
    "            continue\n",
    "        print(f\"Processing {file_path}\")\n",
    "        chunks = generate_chunks(file_path)\n",
    "        for chunk in chunks:\n",
    "            responses = generate_claim_triplet(chunk)\n",
    "            if not responses:\n",
    "                continue\n",
    "            for response in responses:\n",
    "                # response = json.loads(response)\n",
    "                \n",
    "                # print(\"Writing to file\", response)\n",
    "                supports = response.get(\"supporting_claim\", \"N/A\")\n",
    "                contradicts = response.get(\"contradictory_claim\", \"N/A\")\n",
    "                ambiguous = response.get(\"ambiguous_claim\", \"N/A\")\n",
    "            \n",
    "                # Prepare result row\n",
    "                result = {\n",
    "                    'Topic': file_path,\n",
    "                    'Evidence': chunk,\n",
    "                    'Supports': supports,\n",
    "                    'Contradicts': contradicts,\n",
    "                    'Ambiguous': ambiguous\n",
    "                }\n",
    "                \n",
    "                # Write to output file immediately\n",
    "                write_row(output_file, result, file_lock)\n",
    "                \n",
    "                # Write to log file\n",
    "        write_row(log_file, {'processed_file': file_path}, file_lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-5787",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
