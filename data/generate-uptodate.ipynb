{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rateria/anaconda3/envs/cs-5787/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# sanity\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model schema\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from functools import partial\n",
    "from multiprocessing import Lock, Manager, Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.agents import AgentExecutor, Tool, initialize_agent\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.agent_toolkits import JsonToolkit, create_json_agent\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_community.tools.json.tool import JsonSpec\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sn/h8k80m7j0wx704gsf17nmqmm0000gn/T/ipykernel_31536/4272845173.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  oaiembeds = OpenAIEmbeddings(openai_api_key=\"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\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "oaiembeds = OpenAIEmbeddings(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunks(filepath):\n",
    "    with open(filepath) as f:\n",
    "        text = f.read()\n",
    "    single_sentences_list = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    # print (f\"{len(single_sentences_list)} senteneces were found\")\n",
    "    sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
    "    sentences = combine_sentences(sentences)\n",
    "    embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "    distances, sentences = calculate_cosine_distances(sentences)\n",
    "    breakpoint_percentile_threshold = 90\n",
    "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "\n",
    "    # Then we'll see how many distances are actually above this one\n",
    "    num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "    # plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f\"{num_distances_above_theshold + 1} Chunks\");\n",
    "\n",
    "    # Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "    # Initialize the start index\n",
    "    start_index = 0\n",
    "\n",
    "    # Create a list to hold the grouped sentences\n",
    "    chunks = []\n",
    "\n",
    "    # Iterate through the breakpoints to slice the sentences\n",
    "    for index in indices_above_thresh:\n",
    "        # The end index is the current breakpoint\n",
    "        end_index = index\n",
    "\n",
    "        # Slice the sentence_dicts from the current start index to the end index\n",
    "        group = sentences[start_index:end_index + 1]\n",
    "        combined_text = ' '.join([d['sentence'] for d in group])\n",
    "        chunks.append(combined_text)\n",
    "        \n",
    "        # Update the start index for the next group\n",
    "        start_index = index + 1\n",
    "\n",
    "    # The last group, if any sentences remain\n",
    "    if start_index < len(sentences):\n",
    "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "        chunks.append(combined_text)\n",
    "        \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = generate_chunks(\"./UTD2txt/10.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_files(output_file, log_file):\n",
    "    \"\"\"Initialize output and log files with headers if they don't exist.\"\"\"\n",
    "    if not os.path.exists(output_file):\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['Topic', 'Evidence', 'Supports', 'Contradicts', 'Ambiguous'])\n",
    "            writer.writeheader()\n",
    "    \n",
    "    if not os.path.exists(log_file):\n",
    "        with open(log_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['processed_file'])\n",
    "            writer.writeheader()\n",
    "\n",
    "# %%\n",
    "# File paths\n",
    "log_file = os.path.join('.', 'csv', 'uptodate_process_log.csv')\n",
    "output_file = os.path.join('.', 'csv', 'claim_triplets_uptodate.csv')\n",
    "input_file = os.path.join('.', 'csv', 'categorized_content_links_unique.csv')\n",
    "\n",
    "# OpenAI API key\n",
    "# openai_api_key = \"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\"\n",
    "# openai_api_key = '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'\n",
    "openai_api_key = \"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\"\n",
    "# API key for OpenAI (Harshini)\n",
    "# openai_api_key = \"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\"\n",
    "# openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize files\n",
    "init_files(output_file, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_row(file_path, row_dict, file_lock):\n",
    "    \"\"\"Write a single row to the CSV file in a thread-safe manner.\"\"\"\n",
    "    with file_lock:\n",
    "        file_exists = os.path.exists(file_path)\n",
    "        mode = 'a' if file_exists else 'w'\n",
    "        \n",
    "        with open(file_path, mode, newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=list(row_dict.keys()))\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_claim_triplet(summary):\n",
    "    \"\"\"Generate claim triplets from a summary.\"\"\"\n",
    "    try:\n",
    "        prompt = \"\"\"\n",
    "        Using the following detailed summary, generate three types of claims:\\n\\n1) A supporting claim that paraphrases a key assertion.\\n2) A contradictory claim that directly contradicts a key evidence provided in the summary.\\n3) An ambiguous claim that either partially supports or contradicts, or presents elements that are neither clearly supported nor contradicted.\\n\\nEach claim should be one or two sentences long. Ideally, the claims should be generated from different key assertions or sections of the summary.\\n\\nReturn ONLY the claims in this exact JSON format below. DO NOT include any extra text or explanations. DO NOT add ```json``` formatting. Just output the exact JSON as a string.\\n[{{\\n  \\\"supporting_claim\\\": '...',\\n  \\\"contradictory_claim\\\": '...',\\n  \\\"ambiguous_claim\\\": '...'\\n}}].\\n\\n Output {num_samples} triplets of supporting, contradictory, and ambiguous claims from the provided summary. Give all {num_samples} triplets in the same JSON array.\\n\\n\\n\\n\n",
    "\n",
    "        Summary:\n",
    "        {summary}\n",
    "        \"\"\"\n",
    "        # response = agent.run(prompt)\n",
    "        llm = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"summary\", \"additional_details\"],\n",
    "            template=prompt\n",
    "        )\n",
    "        # structured = llm.with_structured_output(Claims)\n",
    "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        response = llm_chain.run({\"summary\": summary, \"num_samples\": 4})\n",
    "        \n",
    "        print(\"JSON Response\", response)\n",
    "        \n",
    "        response = json.loads(response)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating claims for summary: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sn/h8k80m7j0wx704gsf17nmqmm0000gn/T/ipykernel_4897/3403585554.py:17: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "/var/folders/sn/h8k80m7j0wx704gsf17nmqmm0000gn/T/ipykernel_4897/3403585554.py:18: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm_chain.run({\"summary\": summary, \"num_samples\": 4})\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Response [{\n",
      "  \"supporting_claim\": \"Abacavir is classified as a hazardous agent according to NIOSH 2016 guidelines.\",\n",
      "  \"contradictory_claim\": \"Abacavir is not considered a hazardous agent and does not require special handling precautions.\",\n",
      "  \"ambiguous_claim\": \"While Abacavir is labeled hazardous, the specific risks associated with its handling are not clearly defined.\"\n",
      "}, {\n",
      "  \"supporting_claim\": \"Precautions are necessary for the handling and disposal of hazardous agents like Abacavir.\",\n",
      "  \"contradictory_claim\": \"No precautions are needed for the administration of Abacavir as it poses no risk.\",\n",
      "  \"ambiguous_claim\": \"The necessity of precautions may vary based on different handling practices for hazardous agents such as Abacavir.\"\n",
      "}, {\n",
      "  \"supporting_claim\": \"Handling Abacavir requires careful administration and disposal due to its hazardous classification.\",\n",
      "  \"contradictory_claim\": \"It is safe to administer Abacavir without any special precautions.\",\n",
      "  \"ambiguous_claim\": \"The classification of Abacavir as hazardous might not fully capture the complexity of its handling requirements.\"\n",
      "}, {\n",
      "  \"supporting_claim\": \"Appropriate precautions must be taken when dealing with hazardous agents like Abacavir.\",\n",
      "  \"contradictory_claim\": \"Handling hazardous agents does not necessitate any specific precautions for Abacavir.\",\n",
      "  \"ambiguous_claim\": \"The guidelines for hazardous drug handling could be interpreted differently depending on the context of use.\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "out = generate_claim_triplet(chunks[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abacavir is classified as a hazardous agent according to NIOSH 2016 guidelines.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].get(\"supporting_claim\", \"na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files 51\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "manager = Manager()\n",
    "file_lock = manager.Lock()\n",
    "\n",
    "log_df = pd.read_csv(log_file)\n",
    "try:\n",
    "    processed_files = set(log_df[\"processed_file\"])\n",
    "except:\n",
    "    processed_files = []\n",
    "    \n",
    "print(f\"Processed files\", len(processed_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "folder_path = './UTD2txt'\n",
    "\n",
    "def process_file(file_path, output_file, log_file, file_lock):\n",
    "    if file_path in processed_files:\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {file_path}\")\n",
    "    chunks = generate_chunks(file_path)\n",
    "    for chunk in chunks:\n",
    "        responses = generate_claim_triplet(chunk)\n",
    "        if not responses:\n",
    "            continue\n",
    "        for response in responses:\n",
    "            supports = response.get(\"supporting_claim\", \"N/A\")\n",
    "            contradicts = response.get(\"contradictory_claim\", \"N/A\")\n",
    "            ambiguous = response.get(\"ambiguous_claim\", \"N/A\")\n",
    "        \n",
    "            # Prepare result row\n",
    "            result = {\n",
    "                'Topic': file_path,\n",
    "                'Evidence': chunk,\n",
    "                'Supports': supports,\n",
    "                'Contradicts': contradicts,\n",
    "                'Ambiguous': ambiguous\n",
    "            }\n",
    "            \n",
    "            # Write to output file immediately\n",
    "            write_row(output_file, result, file_lock)\n",
    "    \n",
    "    # Write to log file\n",
    "    write_row(log_file, {'processed_file': file_path}, file_lock)\n",
    "\n",
    "def run():\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                futures.append(executor.submit(process_file, file_path, output_file, log_file, file_lock))\n",
    "        \n",
    "        # Optionally wait for all tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the file with the ddg file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the two csv files\n",
    "df1 = pd.read_csv('../data/csv/generated_claim_triplets_with_topics.csv')\n",
    "df2 = pd.read_csv('../data/csv/claim_triplets_uptodate.csv')\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "# Save the concatenated dataframe to a new csv file\n",
    "df.to_csv('../data/csv/combined_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-5787",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
